{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import init #used for initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels,out_channels,stride=1,padding=1,bias=True,groups=1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=padding,bias=bias,groups=groups)\n",
    "\n",
    "def upconv2x2(in_channels,out_channels,mode='transpose'):\n",
    "    if mode=='transpose':\n",
    "        return nn.ConvTranspose2d(in_channels,out_channels,kernel_size=2,stride=2)\n",
    "    else:\n",
    "        return nn.Sequential(nn.Upsample(mode='Bilinear',scale_factor=2),conv1x1(in_channels,out_channels))\n",
    "    \n",
    "def conv1x1(in_channels,out_channels,groups=1):\n",
    "    return nn.Conv2d(in_channels,out_channels,kernel_size=1,groups=groups,stride=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downconv(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,pooling=True):\n",
    "        \n",
    "        super(Downconv,self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "        self.pooling=pooling\n",
    "    \n",
    "        self.conv1=conv3x3(self.in_channels,self.out_channels)\n",
    "        self.conv2=conv3x3(self.out_channels,self.out_channels)\n",
    "    \n",
    "        if self.pooling:\n",
    "            self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=F.relu(self.conv2(x))\n",
    "        before_pool=x\n",
    "        if self.pooling:\n",
    "            x=self.pool(x)\n",
    "        return x,before_pool    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels,out_channels,merge_mode='concat',up_mode='transpose'):\n",
    "        super(UpConv,self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "        self.merge_mode=merge_mode\n",
    "        self.up_mode=up_mode\n",
    "        \n",
    "        self.upconv=upconv2x2(self.in_channels,self.out_channels,mode=self.up_mode)\n",
    "        \n",
    "        if self.merge_mode=='concat':\n",
    "            self.conv1=conv3x3(2*self.out_channels,out_channels)\n",
    "        else:\n",
    "            self.conv1=conv3x3(self.out_channels,self.out_channels)\n",
    "        self.conv2=conv3x3(self.out_channels,self.out_channels)    \n",
    "        \n",
    "    def forward(self,from_down,from_up):\n",
    "        #from up: tensor from decoder path\n",
    "        #from_down tensor to concatenate which is the before_pool tensor in encoder arch\n",
    "        from_up=self.upconv(from_up)\n",
    "        if self.merge_mode=='concat':\n",
    "            x=torch.cat((from_up,from_down),1) #1 gives the dimension to concat along\n",
    "        else:\n",
    "            x=from_up+from_down\n",
    "            \n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=F.relu(self.conv2(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self,num_classes,in_channels=3,depth=5,start_filts=64,up_mode='transpose',merge_mode='concat'):\n",
    "        \n",
    "        super(Unet,self).__init__()\n",
    "        if up_mode in ('transpose','upsample'):\n",
    "            self.up_mode=up_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
    "                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "            \n",
    "        \n",
    "        if merge_mode in ('concat','add'):\n",
    "            self.merge_mode=merge_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"merging. Only \\\"concat\\\" and \"\n",
    "                             \"\\\"add\\\" are allowed.\".format(merge_mode))\n",
    "        \n",
    "        if self.merge_mode=='add' and self.up_mode=='upsample':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
    "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
    "                             \"because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce \"\n",
    "                             \"depth channels (by half).\")\n",
    "            \n",
    "        self.num_classes=num_classes\n",
    "        self.in_channels=in_channels\n",
    "        self.depth=depth\n",
    "        self.start_filts=start_filts\n",
    "        \n",
    "        self.down_convs=[]\n",
    "        self.up_convs=[]\n",
    "        \n",
    "        #create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins=self.in_channels if i==0 else outs\n",
    "            outs=self.start_filts*(2**i)\n",
    "            pooling=True if i<(depth-1) else False\n",
    "\n",
    "            downconv=Downconv(ins,outs,pooling=pooling) \n",
    "            self.down_convs.append(downconv) #list of modules in the down path\n",
    "            \n",
    "        \n",
    "        #create the decoder pathway and add to a list\n",
    "        for i in range(depth-1):\n",
    "            ins=outs\n",
    "            outs=ins//2\n",
    "            up_conv=UpConv(ins,outs,up_mode=up_mode,merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv) #list of modules in the up path \n",
    "            \n",
    "        self.conv_final=conv1x1(outs,num_classes)\n",
    "            \n",
    "        #add the list of modules to the current module\n",
    "        self.down_convs=nn.ModuleList(self.down_convs)\n",
    "        self.up_convs=nn.ModuleList(self.up_convs)\n",
    "            \n",
    "        self.reset_params()\n",
    "            \n",
    "    #def weight_init(m):\n",
    "        #if isinstance(m,nn.Conv2d):\n",
    "            #init.xavier_normal(m.weight.double())\n",
    "            #init.constant(m.bias,0)\n",
    "            \n",
    "    def reset_params(self):\n",
    "        for i,m in enumerate(self.modules()):#self.modules returns an overall iterator over the modules of the net\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                init.xavier_normal(m.weight).double()\n",
    "                init.constant(m.bias,0)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        encoder_outs=[]\n",
    "        \n",
    "        for i,module in enumerate(self.down_convs):\n",
    "            x,before_pool=module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "            \n",
    "        for i,module in enumerate(self.up_convs): \n",
    "            before_pool=encoder_outs[-(i+2)]\n",
    "            x=module(before_pool,x)\n",
    "        \n",
    "        #no softmax but 1x1 conv is used for generating labels\n",
    "        x=self.conv_final(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunalmessi10/anaconda/envs/env1/lib/python2.7/site-packages/ipykernel_launcher.py:68: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/kunalmessi10/anaconda/envs/env1/lib/python2.7/site-packages/ipykernel_launcher.py:69: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    #testing on a random input\n",
    "    model=Unet(3,depth=5,merge_mode='concat')\n",
    "    x=Variable(torch.FloatTensor(np.random.random((1,3,320,320))))\n",
    "    out = model(x)\n",
    "    loss=torch.sum(out)\n",
    "    loss.backward()\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6784.4268)\n"
     ]
    }
   ],
   "source": [
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
